{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Wrapper to get a name for convolution layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d(nn.Module):\n",
    "    \"\"\"Module allows us to save the name of our layers for cache.\"\"\"\n",
    "    def __init__(self, out_channels, kernel_size):\n",
    "        super(Conv1d, self).__init__()\n",
    "        \n",
    "        self.conv = nn.Sequential()\n",
    "        self.conv.add_module(\n",
    "            f\"conv1d_{str(kernel_size)}_{str(out_channels)}\",\n",
    "            nn.Conv1d(1, out_channels, kernel_size)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    \"\"\"Wrapper for embedding to save names in cache.\"\"\"\n",
    "    def __init__(self, vocab_size, word_dim):\n",
    "        super(Embedding, self).__init__()\n",
    "        \n",
    "        self.emb = nn.Sequential()\n",
    "        self.emb.add_module(\n",
    "            f\"embedding_{str(vocab_size)}_{str(word_dim)}\",\n",
    "            nn.Embedding(vocab_size + 2, word_dim, padding_idx=0)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.embedding(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compose multiple convolution layers together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, hparams):\n",
    "        super(CNN, self).__init__()\n",
    "        self.hp = hparams\n",
    "        self.embedding = Embedding(hparams.vocab_size, hparams.word_dim)\n",
    "        self.conv1 = Conv1d(hparams.n_filters1, hparams.kernel1)\n",
    "        self.conv2 = Conv1d(hparams.n_filters2, hparams.kernel2)\n",
    "        self.conv3 = Conv1d(hparams.n_filters3, hparams.kernel3)\n",
    "        self.fc = nn.Linear(self._sum_filters(), 10)\n",
    "        \n",
    "    def _sum_filters(self):\n",
    "        return self.hp.n_filters1 + self.hp.n_filters2 + self.hp.n_filters3\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x).view(-1, 1, self.word_dim * self.max_sent_len)\n",
    "        \n",
    "        conv_results = []\n",
    "        conv_results.append(nn.ReLU(self.conv1(x)).view(-1, self.hp.n_filters1))\n",
    "        conv_results.append(nn.ReLU(self.conv2(x)).view(-1, self.hp.n_filters2))\n",
    "        conv_results.append(nn.ReLU(self.conv2(x)).view(-1, self.hp.n_filters3))\n",
    "        x = torch.cat(conv_results, 1)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperparameters:\n",
    "    kernel1 = 3\n",
    "    kernel2 = 4\n",
    "    kernel3 = 5\n",
    "    n_filters1 = 300\n",
    "    n_filters2 = 300\n",
    "    n_filters3 = 300\n",
    "    vocab_size = 3000\n",
    "    word_dim = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = Hyperparameters()\n",
    "hparams.n_filters1 = 4\n",
    "model = CNN(hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "save = torch.load('model')\n",
    "model = CNN(hparams)\n",
    "model.load_state_dict(save, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.emb.embedding_3000_100.weight\n",
      "conv1.conv.conv1d_3_4.weight\n",
      "conv1.conv.conv1d_3_4.bias\n",
      "conv2.conv.conv1d_4_300.weight\n",
      "conv2.conv.conv1d_4_300.bias\n",
      "conv3.conv.conv1d_5_300.weight\n",
      "conv3.conv.conv1d_5_300.bias\n",
      "fc.weight\n",
      "fc.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new(hyperparameters=None, savefile=None):\n",
    "    \"\"\"Create new MTCNN model.\"\"\"\n",
    "    if hyperparameters:\n",
    "        hparams = hyperparameters\n",
    "    else:\n",
    "        hparams = Hyperparameters()\n",
    "        \n",
    "    model = CNN(hparams)\n",
    "        \n",
    "    if savefile:\n",
    "        model.load_state_dict(savefile, strict=False)\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperspace_update = {\n",
    "    'kernel1': 1,\n",
    "    'kernel2': 2\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams.kernel1 = hyperspace_update['kernel1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN(\n",
       "  (embedding): Embedding(\n",
       "    (emb): Sequential(\n",
       "      (embedding_3000_100): Embedding(3002, 100, padding_idx=0)\n",
       "    )\n",
       "  )\n",
       "  (conv1): Conv1d(\n",
       "    (conv): Sequential(\n",
       "      (conv1d_3_300): Conv1d(1, 300, kernel_size=(3,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (conv2): Conv1d(\n",
       "    (conv): Sequential(\n",
       "      (conv1d_4_300): Conv1d(1, 300, kernel_size=(4,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (conv3): Conv1d(\n",
       "    (conv): Sequential(\n",
       "      (conv1d_5_300): Conv1d(1, 300, kernel_size=(5,), stride=(1,))\n",
       "    )\n",
       "  )\n",
       "  (fc): Linear(in_features=900, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
